{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9aa4fd8c-2218-4681-96d6-ce05687780ca",
   "metadata": {},
   "source": [
    "使用中文语料，通过使用 AWS SageMaker 对 HuggingFace 上开源的中文Llama2模型进行微调 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "26a2d2bd-e233-4f8d-b503-9ccf49c48263",
   "metadata": {},
   "source": [
    "升级最新的到最新的SageMaker。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0391f9c-df6c-426e-9cc6-94d40cf3b80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2622d5ba-437c-40c1-9d19-6c1c60af34a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.15.0 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.0a7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab5f5032-053f-4645-8d14-237b91fd09fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5caaddd-6b4e-4dd3-95ee-69f811c44b27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::890717383483:role/service-role/AmazonSageMaker-ExecutionRole-20231006T171691\n",
      "sagemaker bucket: sagemaker-us-east-1-890717383483\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4751d1d3-8c4a-4957-af8d-e4413ca8781a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed8ea97-cf20-4b2b-a8a0-043eebeba96e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_aws_faq(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the page!\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    faqs = []\n",
    "    qna_blocks = soup.find_all(class_='lb-txt-16 lb-rtxt')  # 此选择器基于AWS的FAQ页面的结构，可能会变更\n",
    "    for block in qna_blocks:\n",
    "        # print(\"Notice!!!\", block)\n",
    "        qa = block.find_all('p')\n",
    "        question_list, answer_list = None, ['Init']  # 保证首次能够进入自动处理循环\n",
    "        for i in qa:\n",
    "            # if i.find('b') or i.find(string=re.compile('问：')) and len(answer_list) >= 1:\n",
    "            if i.find(string=re.compile('问：')) and len(answer_list) >= 1:\n",
    "                faqs.append((question_list, answer_list))\n",
    "                question_list, answer_list = None, []\n",
    "                br_tags = i.find_all('br')\n",
    "                # question = i.get_text(strip=True)\n",
    "                # 针对T4类型相关的问题。 jingamz@\n",
    "                if br_tags:\n",
    "                    question = str(br_tags[0].previous_sibling)\n",
    "                    answer_parts = [str(tag.next_sibling) for tag in br_tags]\n",
    "                    answer = ''.join(answer_parts)\n",
    "                    answer_list.append(answer)\n",
    "                else:\n",
    "                    question = i.get_text(strip=True)\n",
    "                question_list = question\n",
    "                # print(question_list)\n",
    "            else:\n",
    "                answer = i.get_text(strip=True)\n",
    "                answer_list.append(answer)\n",
    "                # print(answer_list)\n",
    "        # 如果 list中只有一条记录，或者处理循环的最后一条记录 @jingamz\n",
    "        faqs.append((question_list, answer_list))\n",
    "    return faqs\n",
    "\n",
    "\n",
    "def get_all_faq(url):\n",
    "    baseurl = 'https://aws.amazon.com/'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the page!\")\n",
    "        return\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    url_blocks = soup.find_all(class_='aws-text-box')\n",
    "    faqs_url = []\n",
    "    flag = 0\n",
    "    for i in url_blocks:\n",
    "        faqurl = i.find('a')\n",
    "        if faqurl is not None:\n",
    "            title = faqurl.get_text(strip=True)\n",
    "            link = faqurl.get('href')\n",
    "            full_url = urljoin(baseurl, link)\n",
    "            # print(title, full_url)\n",
    "            faqs_url.append((title, full_url, flag))\n",
    "    return faqs_url\n",
    "\n",
    "# {\n",
    "#   \"instruction\": \"What is world of warcraft\",\n",
    "#   \"context\": \"\",\n",
    "#   \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "# }\n",
    "\n",
    "jingamz1 = {\n",
    "  \"instruction\": \"什么是jingamz？\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"jingamz是一名来自亚马逊的工程师。目前他主要是负责支持中国北区的ISV客户。jingamz原来工作的地点是PEK12，现在他转换到了PEK7。jingamz曾经还是亚马逊Kindle团队的TAM，目前他从亚马逊的Kindle团队转换到了亚马逊的AWS。\"\n",
    "}\n",
    "jingamz2 = {\n",
    "  \"instruction\": \"谁是jingamz？\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"jingamz是李京在亚马逊的login，通过这个login，可以摘到李京在亚马逊的邮箱以及各种内部的联系方式。在加入到亚马逊之前，jingamz曾经在Motorola以及Oracle工作过。\"\n",
    "}\n",
    "jingamz3 = {\n",
    "  \"instruction\": \"作为一名亚马逊的工程师，jingamz所擅长的技术领域是什么？\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"jingamz具备如下特征掌握的技术和技能非常广泛。技术知识,云服务平台：如AWS、Azure、Google Cloud Platform、阿里云等的熟练掌握。计算：了解虚拟化、容器技术如Docker和Kubernetes。存储：理解对象存储、块存储、文件系统等。网络：基本的网络知识，如VPC、子网、负载均衡、CDN等。开发与运维技能：掌握一到多种编程语言，如Python、Java、Go等。熟悉CI/CD的流程和工具，如Jenkins、Travis CI、CircleCI等。理解IaaS、PaaS、SaaS以及Serverless等模型。使用Infrastructure as Code工具，如Terraform、CloudFormation等。数据与安全：知道如何配置和管理数据库服务，包括SQL和NoSQL。对云安全的理解，包括身份和访问管理、数据加密、网络安全策略等。架构设计：能够设计高可用、高扩展性和容错性的云解决方案。理解微服务架构和API设计。软技能：问题解决：能够迅速定位并解决遇到的技术问题。持续学习：由于云计算技术不断进化，持续学习和适应新技术变得尤为重要。团队协作：能够与其他团队和工程师紧密合作，分享知识和最佳实践。沟通：清晰地与同事、经理和客户交流技术和业务需求。项目管理：了解敏捷开发和DevOps文化。能够评估和优化成本，尤其是在云环境中。行业知识：根据特定行业或应用领域，例如金融、医疗、电信等，可能需要特定的知识或合规性要求。云计算工程师的职责范围广泛，他们需要不断地学习和更新自己的知识。此外，具体的要求可能会根据具体的工作角色、企业和项目而异\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "472c02a7-efc9-40bd-9667-5020f205a831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://aws.amazon.com/cn/ec2/faqs/\"  # EC2\n",
    "url1 = \"https://aws.amazon.com/cn/faqs/\"  # 遍历获得所有服务Faqs的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14592d7b-9a3c-49be-841c-5d8ee0b1a1c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faqs_url = get_all_faq(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c50657-5223-4736-9dff-688eb0fa0f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Amazon EC2 常见问题\n",
      "Faqs link: https://aws.amazon.com/cn/ec2/faqs/\n",
      "Faqs flag: 0\n",
      "{'instruction': '作为一名亚马逊的工程师，jingamz所擅长的技术领域是什么？', 'context': '', 'response': 'jingamz具备如下特征掌握的技术和技能非常广泛。技术知识,云服务平台：如AWS、Azure、Google Cloud Platform、阿里云等的熟练掌握。计算：了解虚拟化、容器技术如Docker和Kubernetes。存储：理解对象存储、块存储、文件系统等。网络：基本的网络知识，如VPC、子网、负载均衡、CDN等。开发与运维技能：掌握一到多种编程语言，如Python、Java、Go等。熟悉CI/CD的流程和工具，如Jenkins、Travis CI、CircleCI等。理解IaaS、PaaS、SaaS以及Serverless等模型。使用Infrastructure as Code工具，如Terraform、CloudFormation等。数据与安全：知道如何配置和管理数据库服务，包括SQL和NoSQL。对云安全的理解，包括身份和访问管理、数据加密、网络安全策略等。架构设计：能够设计高可用、高扩展性和容错性的云解决方案。理解微服务架构和API设计。软技能：问题解决：能够迅速定位并解决遇到的技术问题。持续学习：由于云计算技术不断进化，持续学习和适应新技术变得尤为重要。团队协作：能够与其他团队和工程师紧密合作，分享知识和最佳实践。沟通：清晰地与同事、经理和客户交流技术和业务需求。项目管理：了解敏捷开发和DevOps文化。能够评估和优化成本，尤其是在云环境中。行业知识：根据特定行业或应用领域，例如金融、医疗、电信等，可能需要特定的知识或合规性要求。云计算工程师的职责范围广泛，他们需要不断地学习和更新自己的知识。此外，具体的要求可能会根据具体的工作角色、企业和项目而异'}\n"
     ]
    }
   ],
   "source": [
    "faqs_url = get_all_faq(url1)\n",
    "for title, full_url, flag in faqs_url:\n",
    "    pattern = 'faq'\n",
    "    if re.findall(pattern, full_url):\n",
    "        # 以EC2 FAQs 为例\n",
    "        if full_url == url:\n",
    "            print('Title:', title)\n",
    "            print('Faqs link:', full_url)\n",
    "            print('Faqs flag:', flag)\n",
    "            faqs = fetch_aws_faq(full_url)\n",
    "            qa_list=[]\n",
    "            for q, a in faqs:\n",
    "                my_qa = {}\n",
    "                if q is not None:\n",
    "                    my_qa = {\n",
    "                        \"instruction\": q,\n",
    "                        \"context\": \"\",\n",
    "                        \"response\": ''.join(a)\n",
    "                    }\n",
    "                    qa_list.append(my_qa)\n",
    "            qa_list.append(jingamz1)\n",
    "            qa_list.append(jingamz2)\n",
    "            qa_list.append(jingamz3)\n",
    "print(qa_list[len(qa_list)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b44b78-f5d9-4be5-bc16-8bf3bec03bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3002cb5f-fd1c-4637-8144-f409381d4acc",
   "metadata": {},
   "source": [
    "保证继承自llama2的Tokenizer能够找到【https://arthurchiao.art/blog/llama2-paper-zh/】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fb2ea03-6398-41c7-ad20-15807c76759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b37d032-99bd-4a1e-a3fc-820ec0e53741",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323887c4f9374347bdfebaa9cb80c7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "#注意这种导入Tokenizer的方式是有问题的！！\n",
    "#model_id = \"ziqingyang/chinese-llama-2-7b\"\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#model_id = \"seeledu/Chinese-Llama-2-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "#model_id = \"FlagAlpha/Llama2-Chinese-13b-Chat\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64a33244-1e6c-450e-93ff-aa373a522229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "143e3a29-58a2-4645-87fd-f8133e417781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "问：EC2 Mac 实例是否支持 EBS？\n",
      "\n",
      "### Answer\n",
      "EC2 Mac 实例默认针对 EBS 进行了优化，可以为加密和未加密 EBS 卷提供高达 8 Gbps 的专用 EBS 带宽。\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_prompt(qa_list[randrange(len(qa_list))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ec3ae11-e203-4749-b265-a3567b1d0756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'context', 'response'],\n",
      "    num_rows: 534\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(qa_list)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2d3b59a-f9f1-4364-a88f-10af857348bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "问：价格是否含税？\n",
      "\n",
      "### Answer\n",
      "除非另行说明，否则我们的价格不包含适用的税费和关税（包括增值税和适用的销售税）。使用日本账单地址的客户若要使用 AWS，则需缴纳日本消费税。了解更多。</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 52\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_prompt(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a176de4-277f-4072-9fae-f403c09eef93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-890717383483/processed/llama/ec2qa/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/ec2qa/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2336d9ee-47b7-4967-90be-00bcd0ddb99a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'jingamzec2-chat-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 6,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "343de54a-c3da-40be-adb5-8577a5e0e17d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: jingamzec2-chat-qlora-2023-10-08-14-41--2023-10-08-14-42-06-025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-10-08 14:42:06 Starting - Starting the training job...\n",
      "2023-10-08 14:42:22 Starting - Preparing the instances for training......\n",
      "2023-10-08 14:43:32 Downloading - Downloading input data..........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:42,247 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:42,260 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:42,268 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:42,270 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:43,578 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.31.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 65.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.4.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 14.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.21.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 50.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.40.2 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.5/92.5 MB 25.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 90.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, transformers, accelerate, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.2 peft-0.4.0 safetensors-0.4.0 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:54,207 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:54,207 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:54,238 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:54,261 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:54,284 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:54,295 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 6,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"lr\": 0.0002,\n",
      "        \"merge_weights\": true,\n",
      "        \"model_id\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "        \"per_device_train_batch_size\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"jingamzec2-chat-qlora-2023-10-08-14-41--2023-10-08-14-42-06-025\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-890717383483/jingamzec2-chat-qlora-2023-10-08-14-41--2023-10-08-14-42-06-025/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":6,\"hf_token\":\"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\",\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-chat-hf\",\"per_device_train_batch_size\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-890717383483/jingamzec2-chat-qlora-2023-10-08-14-41--2023-10-08-14-42-06-025/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":6,\"hf_token\":\"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\",\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-chat-hf\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"jingamzec2-chat-qlora-2023-10-08-14-41--2023-10-08-14-42-06-025\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-890717383483/jingamzec2-chat-qlora-2023-10-08-14-41--2023-10-08-14-42-06-025/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"6\",\"--hf_token\",\"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\",\"--lr\",\"0.0002\",\"--merge_weights\",\"True\",\"--model_id\",\"meta-llama/Llama-2-7b-chat-hf\",\"--per_device_train_batch_size\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_WEIGHTS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=meta-llama/Llama-2-7b-chat-hf\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 6 --hf_token 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 --lr 0.0002 --merge_weights True --model_id meta-llama/Llama-2-7b-chat-hf --per_device_train_batch_size 2\u001b[0m\n",
      "\u001b[34m2023-10-08 14:47:54,321 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\n",
      "2023-10-08 14:47:41 Training - Training image download completed. Training in progress.\u001b[34mLogging into the Hugging Face Hub with token hf_rKPXmMd...\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid.\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 614/614 [00:00<00:00, 7.06MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 52.4M/9.98G [00:00<00:21, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 105M/9.98G [00:00<00:20, 476MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 157M/9.98G [00:00<00:20, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 210M/9.98G [00:00<00:20, 484MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 262M/9.98G [00:00<00:20, 482MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 315M/9.98G [00:00<00:19, 488MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▎         | 367M/9.98G [00:00<00:19, 491MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 419M/9.98G [00:00<00:19, 494MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▍         | 472M/9.98G [00:00<00:19, 493MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▌         | 524M/9.98G [00:01<00:19, 492MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 577M/9.98G [00:01<00:19, 494MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▋         | 629M/9.98G [00:01<00:18, 495MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 682M/9.98G [00:01<00:18, 495MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 734M/9.98G [00:01<00:19, 467MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 786M/9.98G [00:01<00:19, 464MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 839M/9.98G [00:01<00:19, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 891M/9.98G [00:01<00:19, 478MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 944M/9.98G [00:01<00:18, 484MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|▉         | 996M/9.98G [00:02<00:18, 487MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.05G/9.98G [00:02<00:18, 485MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.10G/9.98G [00:02<00:19, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.15G/9.98G [00:02<00:18, 466MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.21G/9.98G [00:02<00:18, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.26G/9.98G [00:02<00:18, 474MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.31G/9.98G [00:02<00:18, 464MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▎        | 1.36G/9.98G [00:02<00:18, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 1.42G/9.98G [00:02<00:19, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 1.47G/9.98G [00:03<00:18, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▌        | 1.52G/9.98G [00:03<00:18, 466MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 1.57G/9.98G [00:03<00:17, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▋        | 1.63G/9.98G [00:03<00:17, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.68G/9.98G [00:03<00:18, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.73G/9.98G [00:03<00:17, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.78G/9.98G [00:03<00:17, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.84G/9.98G [00:03<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 1.89G/9.98G [00:04<00:18, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 1.94G/9.98G [00:04<00:18, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 1.99G/9.98G [00:04<00:17, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|██        | 2.04G/9.98G [00:04<00:17, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.10G/9.98G [00:04<00:18, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.15G/9.98G [00:04<00:18, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.20G/9.98G [00:04<00:17, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.25G/9.98G [00:04<00:16, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.31G/9.98G [00:04<00:16, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▎       | 2.36G/9.98G [00:05<00:16, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.41G/9.98G [00:05<00:16, 467MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▍       | 2.46G/9.98G [00:05<00:16, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 2.52G/9.98G [00:05<00:16, 461MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.57G/9.98G [00:05<00:16, 461MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▋       | 2.62G/9.98G [00:05<00:15, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.67G/9.98G [00:05<00:15, 467MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.73G/9.98G [00:05<00:15, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.78G/9.98G [00:05<00:15, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.83G/9.98G [00:06<00:15, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 2.88G/9.98G [00:06<00:15, 464MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 2.94G/9.98G [00:06<00:15, 467MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 2.99G/9.98G [00:06<00:14, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|███       | 3.04G/9.98G [00:06<00:15, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.09G/9.98G [00:06<00:15, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.15G/9.98G [00:06<00:15, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.20G/9.98G [00:06<00:14, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.25G/9.98G [00:06<00:14, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.30G/9.98G [00:07<00:14, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▎      | 3.36G/9.98G [00:07<00:13, 478MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 3.41G/9.98G [00:07<00:13, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▍      | 3.46G/9.98G [00:07<00:13, 482MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▌      | 3.51G/9.98G [00:07<00:13, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 3.57G/9.98G [00:07<00:13, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▋      | 3.62G/9.98G [00:07<00:13, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.67G/9.98G [00:07<00:13, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.72G/9.98G [00:07<00:13, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.77G/9.98G [00:08<00:12, 477MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.83G/9.98G [00:08<00:12, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.88G/9.98G [00:08<00:12, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.93G/9.98G [00:08<00:12, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 3.98G/9.98G [00:08<00:12, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 4.04G/9.98G [00:08<00:12, 461MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 4.09G/9.98G [00:08<00:12, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.14G/9.98G [00:08<00:12, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.19G/9.98G [00:08<00:12, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.25G/9.98G [00:09<00:12, 464MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.30G/9.98G [00:09<00:12, 468MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▎     | 4.35G/9.98G [00:09<00:11, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 4.40G/9.98G [00:09<00:11, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▍     | 4.46G/9.98G [00:09<00:11, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▌     | 4.51G/9.98G [00:09<00:11, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.56G/9.98G [00:09<00:11, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.61G/9.98G [00:09<00:11, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.67G/9.98G [00:09<00:11, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.72G/9.98G [00:10<00:12, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.77G/9.98G [00:10<00:12, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.82G/9.98G [00:10<00:11, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.88G/9.98G [00:10<00:11, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.93G/9.98G [00:10<00:11, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|████▉     | 4.98G/9.98G [00:10<00:10, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 5.03G/9.98G [00:10<00:10, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 5.09G/9.98G [00:10<00:10, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.14G/9.98G [00:11<00:10, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.19G/9.98G [00:11<00:10, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.24G/9.98G [00:11<00:09, 478MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.30G/9.98G [00:11<00:11, 392MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.34G/9.98G [00:11<00:13, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 5.38G/9.98G [00:11<00:14, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 5.43G/9.98G [00:11<00:12, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▍    | 5.48G/9.98G [00:11<00:11, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 5.54G/9.98G [00:12<00:10, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 5.59G/9.98G [00:12<00:11, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.64G/9.98G [00:12<00:10, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.69G/9.98G [00:12<00:10, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.75G/9.98G [00:12<00:10, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.80G/9.98G [00:12<00:09, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▊    | 5.85G/9.98G [00:12<00:09, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.90G/9.98G [00:12<00:09, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|█████▉    | 5.96G/9.98G [00:13<00:09, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|██████    | 6.01G/9.98G [00:13<00:09, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.06G/9.98G [00:13<00:08, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████▏   | 6.11G/9.98G [00:13<00:08, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.17G/9.98G [00:13<00:08, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.22G/9.98G [00:13<00:07, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.27G/9.98G [00:13<00:07, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.32G/9.98G [00:13<00:07, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 6.38G/9.98G [00:13<00:07, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 6.43G/9.98G [00:14<00:07, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▍   | 6.48G/9.98G [00:14<00:07, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 6.53G/9.98G [00:14<00:07, 478MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 6.59G/9.98G [00:14<00:07, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.64G/9.98G [00:14<00:06, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.69G/9.98G [00:14<00:06, 482MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.74G/9.98G [00:14<00:06, 484MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.79G/9.98G [00:14<00:06, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▊   | 6.85G/9.98G [00:14<00:06, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 6.90G/9.98G [00:15<00:06, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 6.95G/9.98G [00:15<00:06, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|███████   | 7.00G/9.98G [00:15<00:06, 482MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 7.06G/9.98G [00:15<00:06, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████▏  | 7.11G/9.98G [00:15<00:05, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.16G/9.98G [00:15<00:06, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.21G/9.98G [00:15<00:06, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.27G/9.98G [00:15<00:06, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.32G/9.98G [00:15<00:06, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.37G/9.98G [00:16<00:06, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.42G/9.98G [00:16<00:06, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 7.48G/9.98G [00:16<00:05, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 7.53G/9.98G [00:16<00:05, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.58G/9.98G [00:16<00:05, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.63G/9.98G [00:16<00:05, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.69G/9.98G [00:16<00:04, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.74G/9.98G [00:16<00:04, 468MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.79G/9.98G [00:17<00:04, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▊  | 7.84G/9.98G [00:17<00:04, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 7.90G/9.98G [00:17<00:04, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 7.95G/9.98G [00:17<00:04, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|████████  | 8.00G/9.98G [00:17<00:04, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.05G/9.98G [00:17<00:04, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.11G/9.98G [00:17<00:04, 466MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.16G/9.98G [00:17<00:03, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.21G/9.98G [00:17<00:03, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.26G/9.98G [00:18<00:03, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.32G/9.98G [00:18<00:03, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.37G/9.98G [00:18<00:03, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [00:18<00:03, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▍ | 8.47G/9.98G [00:18<00:03, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▌ | 8.52G/9.98G [00:18<00:03, 467MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 8.58G/9.98G [00:18<00:02, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.63G/9.98G [00:18<00:02, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.68G/9.98G [00:18<00:02, 474MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.73G/9.98G [00:19<00:02, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.79G/9.98G [00:19<00:02, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▊ | 8.84G/9.98G [00:19<00:02, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 8.89G/9.98G [00:19<00:02, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 8.94G/9.98G [00:19<00:02, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|█████████ | 9.00G/9.98G [00:19<00:02, 474MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.05G/9.98G [00:19<00:01, 477MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.10G/9.98G [00:19<00:01, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.15G/9.98G [00:19<00:01, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.21G/9.98G [00:20<00:01, 482MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.26G/9.98G [00:20<00:01, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.31G/9.98G [00:20<00:01, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.36G/9.98G [00:20<00:01, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.42G/9.98G [00:20<00:01, 478MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 9.47G/9.98G [00:20<00:01, 477MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▌| 9.52G/9.98G [00:20<00:00, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.57G/9.98G [00:20<00:00, 468MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▋| 9.63G/9.98G [00:20<00:00, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.68G/9.98G [00:21<00:00, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.73G/9.98G [00:21<00:00, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.77G/9.98G [00:21<00:00, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.81G/9.98G [00:21<00:00, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 9.87G/9.98G [00:21<00:00, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 9.92G/9.98G [00:21<00:00, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|█████████▉| 9.97G/9.98G [00:22<00:00, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:22<00:00, 452MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:22<00:22, 22.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|▏         | 52.4M/3.50G [00:00<00:07, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 105M/3.50G [00:00<00:07, 472MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 157M/3.50G [00:00<00:07, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 210M/3.50G [00:00<00:07, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 262M/3.50G [00:00<00:06, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 315M/3.50G [00:00<00:06, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 367M/3.50G [00:00<00:06, 477MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 419M/3.50G [00:00<00:06, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 472M/3.50G [00:00<00:06, 481MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 524M/3.50G [00:01<00:06, 483MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▋        | 577M/3.50G [00:01<00:06, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 629M/3.50G [00:01<00:07, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 671M/3.50G [00:01<00:07, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|██        | 713M/3.50G [00:01<00:08, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 765M/3.50G [00:01<00:07, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 807M/3.50G [00:01<00:07, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 849M/3.50G [00:02<00:07, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 891M/3.50G [00:02<00:07, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 933M/3.50G [00:02<00:08, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 986M/3.50G [00:02<00:07, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 1.04G/3.50G [00:02<00:06, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 1.08G/3.50G [00:02<00:06, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 1.13G/3.50G [00:02<00:05, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▎      | 1.17G/3.50G [00:02<00:05, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▌      | 1.23G/3.50G [00:03<00:05, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 1.28G/3.50G [00:03<00:05, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 1.33G/3.50G [00:03<00:05, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 1.38G/3.50G [00:03<00:05, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 1.44G/3.50G [00:03<00:04, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 1.49G/3.50G [00:03<00:04, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 1.54G/3.50G [00:03<00:04, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 1.59G/3.50G [00:03<00:04, 468MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 1.65G/3.50G [00:03<00:03, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▊     | 1.70G/3.50G [00:04<00:03, 477MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 1.75G/3.50G [00:04<00:03, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 1.80G/3.50G [00:04<00:03, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 1.86G/3.50G [00:04<00:03, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▍    | 1.91G/3.50G [00:04<00:03, 468MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 1.96G/3.50G [00:04<00:03, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 2.01G/3.50G [00:04<00:03, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:04<00:03, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 2.12G/3.50G [00:05<00:03, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 2.17G/3.50G [00:05<00:03, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▎   | 2.22G/3.50G [00:05<00:03, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 2.28G/3.50G [00:05<00:02, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 2.33G/3.50G [00:05<00:02, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 2.38G/3.50G [00:05<00:02, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 2.43G/3.50G [00:05<00:02, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 2.49G/3.50G [00:05<00:02, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 2.54G/3.50G [00:05<00:02, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 2.59G/3.50G [00:06<00:02, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 2.64G/3.50G [00:06<00:01, 466MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 2.69G/3.50G [00:06<00:01, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 2.75G/3.50G [00:06<00:01, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 2.80G/3.50G [00:06<00:01, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:06<00:01, 476MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 2.90G/3.50G [00:06<00:01, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 2.96G/3.50G [00:06<00:01, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 3.01G/3.50G [00:07<00:01, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 3.06G/3.50G [00:07<00:00, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 3.11G/3.50G [00:07<00:00, 468MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:07<00:00, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 3.22G/3.50G [00:07<00:00, 475MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 3.27G/3.50G [00:07<00:00, 479MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 3.32G/3.50G [00:07<00:00, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▋| 3.38G/3.50G [00:07<00:00, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 3.43G/3.50G [00:07<00:00, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 3.48G/3.50G [00:08<00:00, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:08<00:00, 433MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:30<00:00, 13.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:30<00:00, 15.11s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json: 100%|██████████| 188/188 [00:00<00:00, 2.37MB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['gate_proj', 'v_proj', 'up_proj', 'o_proj', 'q_proj', 'down_proj', 'k_proj']\u001b[0m\n",
      "\u001b[34mtrainable params: 159,907,840 || all params: 3,660,320,768 || trainable%: 4.368683788535114\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/156 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 1/156 [00:08<20:57,  8.11s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 2/156 [00:16<20:35,  8.02s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/156 [00:24<20:22,  7.99s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/156 [00:31<20:12,  7.98s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 5/156 [00:39<20:03,  7.97s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 6/156 [00:47<19:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 7/156 [00:55<19:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 8/156 [01:03<19:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 9/156 [01:11<19:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 10/156 [01:19<19:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5352, 'learning_rate': 0.0001871794871794872, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m6%|▋         | 10/156 [01:19<19:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 11/156 [01:27<19:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 12/156 [01:35<19:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 13/156 [01:43<18:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 14/156 [01:51<18:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 15/156 [01:59<18:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 16/156 [02:07<18:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 17/156 [02:15<18:25,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 18/156 [02:23<18:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 19/156 [02:31<18:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 20/156 [02:39<18:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3465, 'learning_rate': 0.00017435897435897436, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 20/156 [02:39<18:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 21/156 [02:47<17:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 22/156 [02:55<17:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 23/156 [03:03<17:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 24/156 [03:11<17:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 25/156 [03:19<17:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 26/156 [03:27<17:14,  7.95s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 27/156 [03:34<17:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 28/156 [03:42<16:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 29/156 [03:50<16:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 30/156 [03:58<16:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2128, 'learning_rate': 0.00016153846153846155, 'epoch': 1.15}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 30/156 [03:58<16:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 31/156 [04:06<16:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 32/156 [04:14<16:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 33/156 [04:22<16:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 34/156 [04:30<16:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 35/156 [04:38<16:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 36/156 [04:46<15:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 37/156 [04:54<15:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 38/156 [05:02<15:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 39/156 [05:10<15:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 40/156 [05:18<15:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1186, 'learning_rate': 0.00014871794871794872, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 40/156 [05:18<15:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 41/156 [05:26<15:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 42/156 [05:34<15:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 43/156 [05:42<14:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 44/156 [05:50<14:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 45/156 [05:58<14:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 46/156 [06:06<14:35,  7.96s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 47/156 [06:14<14:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 48/156 [06:22<14:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 49/156 [06:29<14:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 50/156 [06:37<14:03,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0852, 'learning_rate': 0.0001358974358974359, 'epoch': 1.92}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 50/156 [06:37<14:03,  7.96s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 51/156 [06:45<13:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 52/156 [06:53<13:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 53/156 [07:01<13:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 54/156 [07:09<13:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 55/156 [07:17<13:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 56/156 [07:25<13:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 57/156 [07:33<13:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 58/156 [07:41<12:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 59/156 [07:49<12:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 60/156 [07:57<12:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9777, 'learning_rate': 0.0001230769230769231, 'epoch': 2.31}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 60/156 [07:57<12:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 61/156 [08:05<12:35,  7.96s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 62/156 [08:13<12:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 63/156 [08:21<12:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 64/156 [08:29<12:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 65/156 [08:37<12:03,  7.96s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 66/156 [08:45<11:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 67/156 [08:53<11:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 68/156 [09:01<11:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 69/156 [09:09<11:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 70/156 [09:17<11:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9259, 'learning_rate': 0.00011025641025641027, 'epoch': 2.69}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 70/156 [09:17<11:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 71/156 [09:25<11:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 72/156 [09:32<11:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 73/156 [09:40<11:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 74/156 [09:48<10:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 75/156 [09:56<10:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 76/156 [10:04<10:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 77/156 [10:12<10:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 78/156 [10:20<10:20,  7.95s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 79/156 [10:28<10:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 80/156 [10:36<10:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8915, 'learning_rate': 9.743589743589744e-05, 'epoch': 3.08}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 80/156 [10:36<10:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 81/156 [10:44<09:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 82/156 [10:52<09:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 83/156 [11:00<09:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 84/156 [11:08<09:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 85/156 [11:16<09:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 86/156 [11:24<09:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 87/156 [11:32<09:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 88/156 [11:40<09:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 89/156 [11:48<08:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 90/156 [11:56<08:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7417, 'learning_rate': 8.461538461538461e-05, 'epoch': 3.46}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 90/156 [11:56<08:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 91/156 [12:04<08:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 92/156 [12:12<08:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 93/156 [12:20<08:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 94/156 [12:27<08:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 95/156 [12:35<08:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 96/156 [12:43<07:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 97/156 [12:51<07:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 98/156 [12:59<07:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 99/156 [13:07<07:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 100/156 [13:15<07:25,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7564, 'learning_rate': 7.17948717948718e-05, 'epoch': 3.85}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 100/156 [13:15<07:25,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 101/156 [13:23<07:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 102/156 [13:31<07:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 103/156 [13:39<07:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 104/156 [13:47<06:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 105/156 [13:55<06:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 106/156 [14:03<06:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 107/156 [14:11<06:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 108/156 [14:19<06:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 109/156 [14:27<06:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 110/156 [14:35<06:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6398, 'learning_rate': 5.897435897435898e-05, 'epoch': 4.23}\u001b[0m\n",
      "\u001b[34m71%|███████   | 110/156 [14:35<06:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 111/156 [14:43<05:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 112/156 [14:51<05:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 113/156 [14:59<05:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 114/156 [15:07<05:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 115/156 [15:15<05:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 116/156 [15:23<05:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 117/156 [15:30<05:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 118/156 [15:38<05:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 119/156 [15:46<04:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 120/156 [15:54<04:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6141, 'learning_rate': 4.615384615384616e-05, 'epoch': 4.62}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 120/156 [15:54<04:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 121/156 [16:02<04:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 122/156 [16:10<04:30,  7.95s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 123/156 [16:18<04:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 124/156 [16:26<04:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 125/156 [16:34<04:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 126/156 [16:42<03:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 127/156 [16:50<03:50,  7.95s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 128/156 [16:58<03:42,  7.95s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 129/156 [17:06<03:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 130/156 [17:14<03:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5679, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 130/156 [17:14<03:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 131/156 [17:22<03:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 132/156 [17:30<03:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 133/156 [17:38<03:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 134/156 [17:46<02:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 135/156 [17:54<02:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 136/156 [18:02<02:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 137/156 [18:10<02:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 138/156 [18:18<02:23,  7.95s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 139/156 [18:25<02:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 140/156 [18:33<02:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4633, 'learning_rate': 2.0512820512820512e-05, 'epoch': 5.38}\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 140/156 [18:33<02:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 141/156 [18:41<01:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 142/156 [18:49<01:51,  7.95s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 143/156 [18:57<01:43,  7.95s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 144/156 [19:05<01:35,  7.95s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 145/156 [19:13<01:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 146/156 [19:21<01:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 147/156 [19:29<01:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 148/156 [19:37<01:03,  7.96s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 149/156 [19:45<00:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 150/156 [19:53<00:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.498, 'learning_rate': 7.692307692307694e-06, 'epoch': 5.77}\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 150/156 [19:53<00:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 151/156 [20:01<00:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 152/156 [20:09<00:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 153/156 [20:17<00:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 154/156 [20:25<00:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 155/156 [20:33<00:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 156/156 [20:41<00:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1241.2246, 'train_samples_per_second': 0.251, 'train_steps_per_second': 0.126, 'train_loss': 0.8760524743642563, 'epoch': 6.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 156/156 [20:41<00:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 156/156 [20:41<00:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  7.44it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.26it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.50it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 8.16MB/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 198MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 112MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 5.85MB/s]\u001b[0m\n",
      "\u001b[34m2023-10-08 15:12:55,098 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-08 15:12:55,098 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-08 15:12:55,099 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-10-08 15:12:57 Uploading - Uploading generated training model\n",
      "2023-10-08 15:25:39 Completed - Training job completed\n",
      "Training seconds: 2529\n",
      "Billable seconds: 2529\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404984b4-7cf9-4ae3-925b-820811e1656c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
